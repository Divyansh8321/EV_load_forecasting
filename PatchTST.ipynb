{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c975b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import NHITS, NBEATS, PatchTST\n",
    "\n",
    "from neuralforecast.losses.pytorch import MAE\n",
    "from neuralforecast.losses.numpy import mae, mse\n",
    "\n",
    "from datasetsforecast.long_horizon import LongHorizon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f6b510",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/DCFC_load.csv\")\n",
    "load_df = df.drop((['Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6','Unnamed: 7']),axis=1)\n",
    "time = pd.to_datetime(load_df['Time'], format='%d/%m/%y %H:%M')\n",
    "\n",
    "time_train = time[:23328]\n",
    "x_train = load_df[\"PowerkW2\"][:23328]\n",
    "\n",
    "time_cv = time[23328:]\n",
    "x_cv = load_df[\"PowerkW2\"][23328:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e9b8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your existing DataFrame is named load_df\n",
    "# If not, replace 'load_df' with the actual name of your DataFrame\n",
    "load_df['Time'] = pd.to_datetime(load_df['Time'], format='%d/%m/%y %H:%M')\n",
    "\n",
    "# Create a new DataFrame with 15-minute intervals\n",
    "start_time = load_df['Time'].min()\n",
    "end_time = load_df['Time'].max()\n",
    "new_time_range = pd.date_range(start=start_time, end=end_time, freq='15T')\n",
    "new_df = pd.DataFrame({'Time': new_time_range})\n",
    "\n",
    "# Replace the 'Time' column in the original DataFrame with the new one\n",
    "load_df['Time'] = new_df['Time']\n",
    "\n",
    "new_row = {'Time': pd.Timestamp('2022-01-02 00:00:00'), 'PowerkW2': 0.0}\n",
    "\n",
    "# Find the index of the row with the specified timestamp\n",
    "index_of_timestamp = load_df[load_df['Time'] == pd.Timestamp('2022-01-01 23:45:00')].index[0]\n",
    "\n",
    "# Insert the new row after the specified timestamp\n",
    "load_df = pd.concat([load_df.iloc[:index_of_timestamp + 1], pd.DataFrame([new_row], columns=load_df.columns), load_df.iloc[index_of_timestamp + 1:]], ignore_index=True)\n",
    "\n",
    "load_df.drop(96, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e808ed65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>PowerkW2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2022-01-02 09:45:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2022-01-02 10:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2022-01-02 10:15:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2022-01-02 10:30:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2022-01-02 10:45:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2022-01-02 11:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022-01-02 11:15:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2022-01-02 11:30:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2022-01-02 11:45:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2022-01-02 12:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2022-01-02 12:15:00</td>\n",
       "      <td>18.128240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2022-01-02 12:30:00</td>\n",
       "      <td>10.886470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2022-01-02 12:45:00</td>\n",
       "      <td>2.200029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2022-01-02 13:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2022-01-02 13:15:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Time   PowerkW2\n",
       "136 2022-01-02 09:45:00   0.000000\n",
       "137 2022-01-02 10:00:00   0.000000\n",
       "138 2022-01-02 10:15:00   0.000000\n",
       "139 2022-01-02 10:30:00   0.000000\n",
       "140 2022-01-02 10:45:00   0.000000\n",
       "141 2022-01-02 11:00:00   0.000000\n",
       "142 2022-01-02 11:15:00   0.000000\n",
       "143 2022-01-02 11:30:00   0.000000\n",
       "144 2022-01-02 11:45:00   0.000000\n",
       "145 2022-01-02 12:00:00   0.000000\n",
       "146 2022-01-02 12:15:00  18.128240\n",
       "147 2022-01-02 12:30:00  10.886470\n",
       "148 2022-01-02 12:45:00   2.200029\n",
       "149 2022-01-02 13:00:00   0.000000\n",
       "150 2022-01-02 13:15:00   0.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_df['Time'][35040] = pd.Timestamp('2022-12-31 23:45:00')\n",
    "load_df[135:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "962da726",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df[\"PowerkW2\"] = load_df[\"PowerkW2\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3e7a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df['ds'] = pd.to_datetime(load_df['Time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5a2e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df.drop('Time', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bfac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df['unique_id'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f36d714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df.rename(columns={'PowerkW2': 'y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ca1a5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 11712\n",
    "test_size = 23328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5672d92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n",
      "Seed set to 1\n",
      "Seed set to 1\n"
     ]
    }
   ],
   "source": [
    "horizon = 672\n",
    "\n",
    "models = [NHITS(h=horizon,\n",
    "               input_size=2*horizon,\n",
    "               max_steps=10),\n",
    "         NBEATS(h=horizon,\n",
    "               input_size=2*horizon,\n",
    "               max_steps=10),\n",
    "         PatchTST(h=horizon,\n",
    "                 input_size=2*horizon,\n",
    "                 max_steps=10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5487dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = NeuralForecast(models=models, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ff283a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5443b86fb5b24e3893e54339f53209c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m preds_df \u001b[38;5;241m=\u001b[39m nf\u001b[38;5;241m.\u001b[39mcross_validation(df\u001b[38;5;241m=\u001b[39mload_df, val_size\u001b[38;5;241m=\u001b[39mval_size, test_size\u001b[38;5;241m=\u001b[39mtest_size, n_windows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neuralforecast/core.py:520\u001b[0m, in \u001b[0;36mNeuralForecast.cross_validation\u001b[0;34m(self, df, static_df, n_windows, step_size, val_size, test_size, sort_df, use_init_models, verbose, **data_kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m fcsts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull(\n\u001b[1;32m    516\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mn_groups \u001b[38;5;241m*\u001b[39m h \u001b[38;5;241m*\u001b[39m n_windows, \u001b[38;5;28mlen\u001b[39m(cols)), np\u001b[38;5;241m.\u001b[39mnan, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    517\u001b[0m )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels:\n\u001b[0;32m--> 520\u001b[0m     model\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, val_size\u001b[38;5;241m=\u001b[39mval_size, test_size\u001b[38;5;241m=\u001b[39mtest_size)\n\u001b[1;32m    521\u001b[0m     model_fcsts \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(\n\u001b[1;32m    522\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, step_size\u001b[38;5;241m=\u001b[39mstep_size, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata_kwargs\n\u001b[1;32m    523\u001b[0m     )\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;66;03m# Append predictions in memory placeholder\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neuralforecast/common/_base_windows.py:734\u001b[0m, in \u001b[0;36mBaseWindows.fit\u001b[0;34m(self, dataset, val_size, test_size, random_seed)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_val_every_n_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    733\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer_kwargs)\n\u001b[0;32m--> 734\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1033\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1062\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1062\u001b[0m val_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1064\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loop_run(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, hook_name, \u001b[38;5;241m*\u001b[39mstep_args)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:403\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 403\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mvalidation_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neuralforecast/common/_base_windows.py:574\u001b[0m, in \u001b[0;36mBaseWindows.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    565\u001b[0m windows_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    566\u001b[0m     insample_y\u001b[38;5;241m=\u001b[39minsample_y,  \u001b[38;5;66;03m# [Ws, L]\u001b[39;00m\n\u001b[1;32m    567\u001b[0m     insample_mask\u001b[38;5;241m=\u001b[39minsample_mask,  \u001b[38;5;66;03m# [Ws, L]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    570\u001b[0m     stat_exog\u001b[38;5;241m=\u001b[39mstat_exog,\n\u001b[1;32m    571\u001b[0m )  \u001b[38;5;66;03m# [Ws, 1]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;66;03m# Model Predictions\u001b[39;00m\n\u001b[0;32m--> 574\u001b[0m output_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(windows_batch)\n\u001b[1;32m    575\u001b[0m valid_loss_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_valid_loss(\n\u001b[1;32m    576\u001b[0m     outsample_y\u001b[38;5;241m=\u001b[39moriginal_outsample_y,\n\u001b[1;32m    577\u001b[0m     output\u001b[38;5;241m=\u001b[39moutput_batch,\n\u001b[1;32m    578\u001b[0m     outsample_mask\u001b[38;5;241m=\u001b[39moutsample_mask,\n\u001b[1;32m    579\u001b[0m     temporal_cols\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemporal_cols\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    580\u001b[0m )\n\u001b[1;32m    581\u001b[0m valid_losses\u001b[38;5;241m.\u001b[39mappend(valid_loss_batch)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neuralforecast/models/nhits.py:392\u001b[0m, in \u001b[0;36mNHITS.forward\u001b[0;34m(self, windows_batch)\u001b[0m\n\u001b[1;32m    390\u001b[0m block_forecasts \u001b[38;5;241m=\u001b[39m [forecast\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh, \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m--> 392\u001b[0m     backcast, block_forecast \u001b[38;5;241m=\u001b[39m block(\n\u001b[1;32m    393\u001b[0m         insample_y\u001b[38;5;241m=\u001b[39mresiduals,\n\u001b[1;32m    394\u001b[0m         futr_exog\u001b[38;5;241m=\u001b[39mfutr_exog,\n\u001b[1;32m    395\u001b[0m         hist_exog\u001b[38;5;241m=\u001b[39mhist_exog,\n\u001b[1;32m    396\u001b[0m         stat_exog\u001b[38;5;241m=\u001b[39mstat_exog,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    398\u001b[0m     residuals \u001b[38;5;241m=\u001b[39m (residuals \u001b[38;5;241m-\u001b[39m backcast) \u001b[38;5;241m*\u001b[39m insample_mask\n\u001b[1;32m    399\u001b[0m     forecast \u001b[38;5;241m=\u001b[39m forecast \u001b[38;5;241m+\u001b[39m block_forecast\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neuralforecast/models/nhits.py:179\u001b[0m, in \u001b[0;36mNHITSBlock.forward\u001b[0;34m(self, insample_y, futr_exog, hist_exog, stat_exog)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Compute local projection weights and projection\u001b[39;00m\n\u001b[1;32m    178\u001b[0m theta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers(insample_y)\n\u001b[0;32m--> 179\u001b[0m backcast, forecast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasis(theta)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m backcast, forecast\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/neuralforecast/models/nhits.py:43\u001b[0m, in \u001b[0;36m_IdentityBasis.forward\u001b[0;34m(self, theta)\u001b[0m\n\u001b[1;32m     40\u001b[0m knots \u001b[38;5;241m=\u001b[39m knots\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mlen\u001b[39m(knots), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation_mode \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# knots = knots[:,None,:]\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     forecast \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m     44\u001b[0m         knots, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecast_size, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation_mode\n\u001b[1;32m     45\u001b[0m     )\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# forecast = forecast[:,0,:]\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcubic\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation_mode:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3954\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3953\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_linear1d(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[1;32m   3955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3956\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "preds_df = nf.cross_validation(df=load_df, val_size=val_size, test_size=test_size, n_windows=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796f3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = preds_df['y'].values\n",
    "y_pred_nhits = preds_df['NHITS'].values\n",
    "y_pred_nbeats = preds_df['NBEATS'].values\n",
    "y_pred_patchtst = preds_df['PatchTST'].values\n",
    "\n",
    "n_series = len(Y_df['unique_id'].unique())\n",
    "\n",
    "y_true = y_true.reshape(n_series, -1, horizon)\n",
    "y_pred_nhits = y_pred_nhits.reshape(n_series, -1, horizon)\n",
    "y_pred_nbeats = y_pred_nbeats.reshape(n_series, -1, horizon)\n",
    "y_pred_patchtst = y_pred_patchtst.reshape(n_series, -1, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.plot(y_true[0, 0, :], label='True')\n",
    "ax.plot(y_pred_nhits[0, 0, :], label='N-HiTS', ls='--')\n",
    "ax.plot(y_pred_nbeats[0, 0, :], label='N-BEATS', ls=':')\n",
    "ax.plot(y_pred_patchtst[0, 0, :], label='PatchTST', ls='-.')\n",
    "ax.set_ylabel('Exchange rate')\n",
    "ax.set_xlabel('Forecast horizon')\n",
    "ax.legend(loc='best')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cba570",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'N-HiTS': [mae(y_pred_nhits, y_true), mse(y_pred_nhits, y_true)],\n",
    "       'N-BEATS': [mae(y_pred_nbeats, y_true), mse(y_pred_nbeats, y_true)],\n",
    "       'PatchTST': [mae(y_pred_patchtst, y_true), mse(y_pred_patchtst, y_true)]}\n",
    "\n",
    "metrics_df = pd.DataFrame(data=data)\n",
    "metrics_df.index = ['mae', 'mse']\n",
    "\n",
    "metrics_df.style.highlight_min(color='lightgreen', axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting_env",
   "language": "python",
   "name": "forecasting_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
