{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace0b500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts import TimeSeries\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00b198f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/DCFC_load.csv\")\n",
    "load_df = df.drop((['Unnamed: 2','Unnamed: 3','Unnamed: 4','Unnamed: 5','Unnamed: 6','Unnamed: 7']),axis=1)\n",
    "time = pd.to_datetime(load_df['Time'], format='%d/%m/%y %H:%M')\n",
    "\n",
    "time_train = time[:23328]\n",
    "x_train = load_df[\"PowerkW2\"][:23328]\n",
    "\n",
    "time_cv = time[23328:]\n",
    "x_cv = load_df[\"PowerkW2\"][23328:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f496e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your existing DataFrame is named load_df\n",
    "# If not, replace 'load_df' with the actual name of your DataFrame\n",
    "load_df['Time'] = pd.to_datetime(load_df['Time'], format='%d/%m/%y %H:%M')\n",
    "\n",
    "# Create a new DataFrame with 15-minute intervals\n",
    "start_time = load_df['Time'].min()\n",
    "end_time = load_df['Time'].max()\n",
    "new_time_range = pd.date_range(start=start_time, end=end_time, freq='15T')\n",
    "new_df = pd.DataFrame({'Time': new_time_range})\n",
    "\n",
    "# Replace the 'Time' column in the original DataFrame with the new one\n",
    "load_df['Time'] = new_df['Time']\n",
    "\n",
    "new_row = {'Time': pd.Timestamp('2022-01-02 00:00:00'), 'PowerkW2': 0.0}\n",
    "\n",
    "# Find the index of the row with the specified timestamp\n",
    "index_of_timestamp = load_df[load_df['Time'] == pd.Timestamp('2022-01-01 23:45:00')].index[0]\n",
    "\n",
    "# Insert the new row after the specified timestamp\n",
    "load_df = pd.concat([load_df.iloc[:index_of_timestamp + 1], pd.DataFrame([new_row], columns=load_df.columns), load_df.iloc[index_of_timestamp + 1:]], ignore_index=True)\n",
    "\n",
    "load_df.drop(96, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e6bc331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>PowerkW2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2022-01-02 09:45:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2022-01-02 10:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2022-01-02 10:15:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>2022-01-02 10:30:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>2022-01-02 10:45:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>2022-01-02 11:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>2022-01-02 11:15:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>2022-01-02 11:30:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>2022-01-02 11:45:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2022-01-02 12:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>2022-01-02 12:15:00</td>\n",
       "      <td>18.128240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>2022-01-02 12:30:00</td>\n",
       "      <td>10.886470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2022-01-02 12:45:00</td>\n",
       "      <td>2.200029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2022-01-02 13:00:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2022-01-02 13:15:00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Time   PowerkW2\n",
       "136 2022-01-02 09:45:00   0.000000\n",
       "137 2022-01-02 10:00:00   0.000000\n",
       "138 2022-01-02 10:15:00   0.000000\n",
       "139 2022-01-02 10:30:00   0.000000\n",
       "140 2022-01-02 10:45:00   0.000000\n",
       "141 2022-01-02 11:00:00   0.000000\n",
       "142 2022-01-02 11:15:00   0.000000\n",
       "143 2022-01-02 11:30:00   0.000000\n",
       "144 2022-01-02 11:45:00   0.000000\n",
       "145 2022-01-02 12:00:00   0.000000\n",
       "146 2022-01-02 12:15:00  18.128240\n",
       "147 2022-01-02 12:30:00  10.886470\n",
       "148 2022-01-02 12:45:00   2.200029\n",
       "149 2022-01-02 13:00:00   0.000000\n",
       "150 2022-01-02 13:15:00   0.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_df['Time'][35040] = pd.Timestamp('2022-12-31 23:45:00')\n",
    "load_df[135:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd9fbe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_df[\"PowerkW2\"] = load_df[\"PowerkW2\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d1f0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = TimeSeries.from_dataframe(load_df, time_col='Time', freq = '15T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fbca823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import NHiTSModel\n",
    "from darts.dataprocessing.transformers import Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b32e1e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Time'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train, test = series[:-2976], series[-2976:]\n",
    "\n",
    "train.plot(label='train')\n",
    "test.plot(label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e424036",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaler = Scaler()\n",
    "scaled_train = train_scaler.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8efacf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "nhits = NHiTSModel(\n",
    "    input_chunk_length=672, \n",
    "    output_chunk_length=96,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e035f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | stacks        | ModuleList       | 1.6 M \n",
      "---------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "344 K     Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.488     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5870a3d3004c8ea6889fdf494f3fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                                   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NotImplementedError",
     "evalue": "The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nhits\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      2\u001b[0m     scaled_train,\n\u001b[1;32m      3\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/utils/torch.py:112\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[1;32m    111\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decorated(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/torch_forecasting_model.py:727\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit\u001b[0;34m(self, series, past_covariates, future_covariates, val_series, val_past_covariates, val_future_covariates, trainer, verbose, epochs, max_samples_per_ts, num_loader_workers)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;66;03m# call super fit only if user is actually fitting the model\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m    723\u001b[0m     series\u001b[38;5;241m=\u001b[39mseq2series(series),\n\u001b[1;32m    724\u001b[0m     past_covariates\u001b[38;5;241m=\u001b[39mseq2series(past_covariates),\n\u001b[1;32m    725\u001b[0m     future_covariates\u001b[38;5;241m=\u001b[39mseq2series(future_covariates),\n\u001b[1;32m    726\u001b[0m )\n\u001b[0;32m--> 727\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_from_dataset(\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/utils/torch.py:112\u001b[0m, in \u001b[0;36mrandom_method.<locals>.decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fork_rng():\n\u001b[1;32m    111\u001b[0m     manual_seed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_random_instance\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, high\u001b[38;5;241m=\u001b[39mMAX_TORCH_SEED_VALUE))\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decorated(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/torch_forecasting_model.py:929\u001b[0m, in \u001b[0;36mTorchForecastingModel.fit_from_dataset\u001b[0;34m(self, train_dataset, val_dataset, trainer, verbose, epochs, num_loader_workers)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;129m@random_method\u001b[39m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_from_dataset\u001b[39m(\n\u001b[1;32m    879\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m     num_loader_workers: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    886\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorchForecastingModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    887\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    Train the model with a specific :class:`darts.utils.data.TrainingDataset` instance.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03m    These datasets implement a PyTorch ``Dataset``, and specify how the target and covariates are sliced\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;124;03m        Fitted model.\u001b[39;00m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[1;32m    930\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_for_train(\n\u001b[1;32m    931\u001b[0m             train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    932\u001b[0m             val_dataset\u001b[38;5;241m=\u001b[39mval_dataset,\n\u001b[1;32m    933\u001b[0m             trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[1;32m    934\u001b[0m             verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    935\u001b[0m             epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m    936\u001b[0m             num_loader_workers\u001b[38;5;241m=\u001b[39mnum_loader_workers,\n\u001b[1;32m    937\u001b[0m         )\n\u001b[1;32m    938\u001b[0m     )\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/torch_forecasting_model.py:1073\u001b[0m, in \u001b[0;36mTorchForecastingModel._train\u001b[0;34m(self, trainer, model, train_loader, val_loader)\u001b[0m\n\u001b[1;32m   1070\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ckpt_path\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1073\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m   1074\u001b[0m     model,\n\u001b[1;32m   1075\u001b[0m     train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m   1076\u001b[0m     val_dataloaders\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[1;32m   1077\u001b[0m     ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path,\n\u001b[1;32m   1078\u001b[0m )\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m   1080\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m trainer\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:544\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 544\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_and_handle_interrupt(\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    546\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:580\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    574\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    576\u001b[0m     ckpt_path,\n\u001b[1;32m    577\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    579\u001b[0m )\n\u001b[0;32m--> 580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(model, ckpt_path\u001b[38;5;241m=\u001b[39mckpt_path)\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_stage()\n\u001b[1;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1035\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_sanity_check()\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1035\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance()\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:359\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch_loop\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:136\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(data_fetcher)\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:240\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_optimization\u001b[38;5;241m.\u001b[39mrun(trainer\u001b[38;5;241m.\u001b[39moptimizers[\u001b[38;5;241m0\u001b[39m], batch_idx, kwargs)\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m         batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_optimization\u001b[38;5;241m.\u001b[39mrun(kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:187\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         closure()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step(batch_idx, closure)\n\u001b[1;32m    189\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:265\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_ready()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_lightning_module_hook(\n\u001b[1;32m    266\u001b[0m     trainer,\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_step\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    268\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mcurrent_epoch,\n\u001b[1;32m    269\u001b[0m     batch_idx,\n\u001b[1;32m    270\u001b[0m     optimizer,\n\u001b[1;32m    271\u001b[0m     train_step_and_backward_closure,\n\u001b[1;32m    272\u001b[0m )\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim_progress\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep\u001b[38;5;241m.\u001b[39mincrement_completed()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:157\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 157\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    160\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1282\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimizer_step\u001b[39m(\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1245\u001b[0m     epoch: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1249\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m    the optimizer.\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \n\u001b[1;32m   1281\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1282\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39moptimizer_closure)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:151\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy\u001b[38;5;241m.\u001b[39moptimizer_step(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer, closure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:230\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 230\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39moptimizer_step(optimizer, model\u001b[38;5;241m=\u001b[39mmodel, closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:117\u001b[0m, in \u001b[0;36mPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[0;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimizer\u001b[38;5;241m.\u001b[39mstep(closure\u001b[38;5;241m=\u001b[39mclosure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/optim/adam.py:121\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m closure \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 121\u001b[0m         loss \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    124\u001b[0m     params_with_grad \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:104\u001b[0m, in \u001b[0;36mPrecision._wrap_closure\u001b[0;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrap_closure\u001b[39m(\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     93\u001b[0m     model: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpl.LightningModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[1;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[1;32m     96\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    hook is called.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m     closure_result \u001b[38;5;241m=\u001b[39m closure()\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_closure(model, optimizer)\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closure_result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclosure(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:126\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 126\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_fn()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarning_cache\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m call\u001b[38;5;241m.\u001b[39m_call_strategy_hook(trainer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_result_cls\u001b[38;5;241m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[38;5;241m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:382\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 382\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/pl_forecasting_module.py:209\u001b[0m, in \u001b[0;36mPLForecastingModule.training_step\u001b[0;34m(self, train_batch, batch_idx)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_batch, batch_idx) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"performs the training step\"\"\"\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_produce_train_output(train_batch[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    210\u001b[0m     target \u001b[38;5;241m=\u001b[39m train_batch[\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    212\u001b[0m     ]  \u001b[38;5;66;03m# By convention target is always the last element returned by datasets\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_loss(output, target)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/pl_forecasting_module.py:573\u001b[0m, in \u001b[0;36mPLPastCovariatesModule._produce_train_output\u001b[0;34m(self, input_batch)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# Currently all our PastCovariates models require past target and covariates concatenated\u001b[39;00m\n\u001b[1;32m    567\u001b[0m inpt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    568\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat([past_target, past_covariates], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_covariates \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m past_target,\n\u001b[1;32m    571\u001b[0m     static_covariates,\n\u001b[1;32m    572\u001b[0m )\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(inpt)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/pl_forecasting_module.py:47\u001b[0m, in \u001b[0;36mio_processor.<locals>.forward_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_reversible_instance_norm:\n\u001b[0;32m---> 47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m forward(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# x is input batch tuple which by definition has the past features in the first element starting with the\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;66;03m# first n target features\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     x: Tuple \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/nhits.py:444\u001b[0m, in \u001b[0;36m_NHiTSModule.forward\u001b[0;34m(self, x_in)\u001b[0m\n\u001b[1;32m    434\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    435\u001b[0m     x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_chunk_length_multi,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    439\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    440\u001b[0m )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stack \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstacks_list:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# compute stack output\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m     stack_residual, stack_forecast \u001b[38;5;241m=\u001b[39m stack(x)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# add stack forecast to final output\u001b[39;00m\n\u001b[1;32m    447\u001b[0m     y \u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m+\u001b[39m stack_forecast\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/nhits.py:308\u001b[0m, in \u001b[0;36m_Stack.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    298\u001b[0m stack_forecast \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    299\u001b[0m     x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_chunk_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    304\u001b[0m )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks_list:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;66;03m# pass input through block\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m     x_hat, y_hat \u001b[38;5;241m=\u001b[39m block(x)\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# add block forecast to stack forecast\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     stack_forecast \u001b[38;5;241m=\u001b[39m stack_forecast \u001b[38;5;241m+\u001b[39m y_hat\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/darts/models/forecasting/nhits.py:195\u001b[0m, in \u001b[0;36m_Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    192\u001b[0m theta_backcast \u001b[38;5;241m=\u001b[39m theta_backcast\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# interpolate both backcast and forecast from the thetas\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m    196\u001b[0m     theta_backcast, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_chunk_length, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m )\n\u001b[1;32m    198\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(\n\u001b[1;32m    199\u001b[0m     theta_forecast, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_chunk_length, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m x_hat\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# remove 2nd dim we added before interpolation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3954\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3952\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3953\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mupsample_linear1d(\u001b[38;5;28minput\u001b[39m, output_size, align_corners, scale_factors)\n\u001b[1;32m   3955\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3956\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: The operator 'aten::upsample_linear1d.out' is not currently implemented for the MPS device. If you want this op to be added in priority during the prototype phase of this feature, please comment on https://github.com/pytorch/pytorch/issues/77764. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
     ]
    }
   ],
   "source": [
    "nhits.fit(\n",
    "    scaled_train,\n",
    "    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d1995",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pred_nhits = nhits.predict(n=2976)\n",
    "\n",
    "pred_nhits = train_scaler.inverse_transform(scaled_pred_nhits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1f8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_nhits = mae(test, pred_nhits)\n",
    "\n",
    "print(mae_nbeats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting_env",
   "language": "python",
   "name": "forecasting_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
